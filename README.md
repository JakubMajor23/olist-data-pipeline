<div align="center">

# Olist E-commerce Data Pipeline

![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7%2B-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![dbt](https://img.shields.io/badge/dbt--Core-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker--Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![SQLFluff](https://img.shields.io/badge/SQLFluff-Expected_Quality-00C7B7?style=for-the-badge&logo=sql&logoColor=white)

<br>

**Kompletny, skalowalny system ELT (Extract, Load, Transform) symulujÄ…cy Å›rodowisko produkcyjne e-commerce.**

[Kontekst](#-kontekst-i-cele) â€¢
[Technologie](#-zastosowane-wzorce-i-technologie) â€¢
[Architektura](#-architektura-i-przepÅ‚yw-danych) â€¢
[Uruchomienie](#-instrukcja-uruchomienia)

</div>

---

## ðŸ’¡ Kontekst i Cele

**Problem:** Surowe dane Olist to rozproszone logi transakcyjne â€“ analiza przychodu, opÃ³ÅºnieÅ„ czy retencji wymaga Å‚Ä…czenia wielu tabel i jest nieefektywna w czasie rzeczywistym.

**RozwiÄ…zanie:** Zautomatyzowany potok danych przeksztaÅ‚cajÄ…cy surowe logi w czysty model **Galaxy Schema** (Konstelacja FaktÃ³w) w Hurtowni Danych.

### GÅ‚Ã³wne Cele
- **Single Source of Truth:** Centralizacja danych o ZamÃ³wieniach, PÅ‚atnoÅ›ciach i Produktach.
- **SkalowalnoÅ›Ä‡:** Åadowanie przyrostowe (Incremental Loading) dla obsÅ‚ugi rosnÄ…cego wolumenu danych.
- **JakoÅ›Ä‡ Danych:** IntegralnoÅ›Ä‡ referencyjna, brak duplikatÃ³w (IdempotentnoÅ›Ä‡) i testy `dbt`.

---

## Zastosowane Wzorce i Technologie

Projekt realizuje zasady inÅ¼ynierii danych (**Modern Data Stack**) poprzez:

| Obszar | Implementacja |
| :--- | :--- |
| **Orkiestracja** | **Event-Driven Airflow**: Wyzwalanie DAG-Ã³w przez REST API zaraz po pojawieniu siÄ™ nowych danych (symulacja). |
| **Modelowanie** | **Galaxy Schema**: Architektura Konstelacji FaktÃ³w (3 tabele faktÃ³w) eliminujÄ…ca problem *Fan-out* i iloczynu kartezjaÅ„skiego. |
| **Transformacja** | **dbt Core**: Modele zmaterializowane jako `incremental` oraz `table`, makra Jinja (DRY), testy jakoÅ›ci danych. |
| **JakoÅ›Ä‡ Kodu** | **SQLFluff**: Linter SQL zapewniajÄ…cy spÃ³jny styl kodu (zgodnie z plikiem `.sqlfluff`). |
| **Infrastruktura** | **Docker & Docker Compose**: PeÅ‚na konteneryzacja Airflow (z dbt) oraz bazy danych Postgres. |

---

## Architektura i PrzepÅ‚yw Danych

System zaprojektowano moduÅ‚owo, oddzielajÄ…c warstwÄ™ symulacji od wÅ‚aÅ›ciwego przetwarzania.

### Cykl Å»ycia Danych (End-to-End Flow)

Proces symuluje rzeczywiste dziaÅ‚anie hurtowni danych w trybie przyrostowym (Incremental Load):

1.  **Symulacja Transakcji (`simulate_production.py`):**
    * Skrypt pobiera dane z plikÃ³w CSV odpowiadajÄ…ce konkretnemu miesiÄ…cowi (np. styczeÅ„ 2017).
    * Dane sÄ… Å‚adowane do operacyjnej bazy danych (`postgres-olist-source`), zachowujÄ…c wiÄ™zy integralnoÅ›ci (najpierw Klienci, potem ZamÃ³wienia, na koÅ„cu PÅ‚atnoÅ›ci/Recenzje).

2.  **Trigger API (`run_demo.py`):**
    * Natychmiast po zaÅ‚adowaniu danych, orkiestrator wysyÅ‚a zapytanie POST do REST API Airflow.
    * Przekazuje parametr `logical_date`, co pozwala na precyzyjne przetworzenie tylko nowego wycinka czasu.

3.  **Extract & Load (Airflow DAG):**
    * **IdempotentnoÅ›Ä‡:** Przed zaÅ‚adowaniem, DAG usuwa z warstwy `raw_data` wszelkie dane dla przetwarzanego miesiÄ…ca. Zapobiega to duplikatom w przypadku ponownego uruchomienia.
    * **Transfer:** Dane sÄ… przenoszone z bazy ÅºrÃ³dÅ‚owej do hurtowni (Raw Layer) przy uÅ¼yciu wydajnych silnikÃ³w SQLAlchemy.

4.  **Transformacja (dbt):**
    * Airflow uruchamia kontener z dbt (`dbt run`).
    * Dane surowe sÄ… czyszczone (Staging) i modelowane do postaci tabel faktÃ³w i wymiarÃ³w (Marts).

5.  **Walidacja:**
    * Na koÅ„cu `run_demo.py` uruchamiany jest skrypt weryfikujÄ…cy zgodnoÅ›Ä‡ liczby wierszy miÄ™dzy ÅºrÃ³dÅ‚em a hurtowniÄ….
---
## ðŸ› ï¸ SzczegÃ³Å‚y Transformacji (dbt)

Warstwa transformacji zostaÅ‚a podzielona na dwa etapy zgodnie z dobrymi praktykami Analytics Engineering:

### Warstwa Staging (Raw -> Staging)
* Materializacja jako `incremental` dla duÅ¼ych tabel (ZamÃ³wienia, PÅ‚atnoÅ›ci) i `table` dla sÅ‚ownikÃ³w.
* Logika **Fail Fast**: Plik `dbt_project.yml` wymusza testy unikalnoÅ›ci kluczy podstawowych.

### Warstwa Marts (Staging -> Facts/Dims)
Model **Galaxy Schema** Å‚Ä…czy procesy biznesowe przez wspÃ³lne wymiary (*Conformed Dimensions*).

| Tabela FaktÃ³w | Opis i Logika |
| :--- | :--- |
| **fact_orders** | Centralna tabela transakcyjna. Agreguje wartoÅ›ci koszyka (`SUM(price)`), koszty dostawy oraz Å‚Ä…czy statusy zamÃ³wieÅ„ i recenzje w jeden widok analityczny. |
| **fact_sales_items** | Najbardziej granularna tabela (poziom produktu w koszyku). Pozwala na analizÄ™ sprzedaÅ¼y per Produkt (`product_id`) i Sprzedawca (`seller_id`). |
| **fact_payments** | Analiza przepÅ‚ywÃ³w pieniÄ™Å¼nych, typÃ³w pÅ‚atnoÅ›ci (karta, voucher) oraz rat (`payment_installments`). |

---

## ðŸŒŸ WyrÃ³Å¼niajÄ…ce RozwiÄ…zania Techniczne

Projekt implementuje zaawansowane wzorce inÅ¼ynieryjne, wykraczajÄ…ce poza standardowe kursy ETL:

### 1. Zaawansowane Modelowanie (Ghost Records)
W tabelach wymiarÃ³w (np. `dim_products`, `dim_reviews`) zastosowano tzw. **Ghost Records**.
* **Problem:** Brak spÃ³jnoÅ›ci referencyjnej (np. zamÃ³wienie produktu, ktÃ³rego nie ma w bazie produktÃ³w) powoduje utratÄ™ wierszy przy `INNER JOIN`.
* **RozwiÄ…zanie:** Sztuczny rekord z kluczem `MD5('unknown')`. BÅ‚Ä™dne klucze obce sÄ… mapowane do kategorii "Unknown" zamiast byÄ‡ odrzucane, co gwarantuje kompletnoÅ›Ä‡ raportÃ³w finansowych.

### 2. JakoÅ›Ä‡ Danych Geograficznych (Data Cleaning)
Surowe dane logistyczne zawierajÄ… wiele bÅ‚Ä™dÃ³w (np. koordynaty poza granicami Brazylii) oraz duplikatÃ³w (wiele odczytÃ³w GPS dla jednego kodu pocztowego).
* **Walidacja:** Filtrowanie koordynatÃ³w w warstwie Staging.
* **Agregacja:** Wyliczanie centroidu (Å›rednia szerokoÅ›Ä‡/dÅ‚ugoÅ›Ä‡) dla kaÅ¼dego `zip_code` w celu stworzenia unikalnego sÅ‚ownika lokalizacji.

### 3. ObsÅ‚uga Historii KlientÃ³w (Deduplication)
Klienci w systemie Olist mogÄ… zmieniaÄ‡ adresy.
* **Logika:** Wymiar `dim_customers` wykorzystuje funkcjÄ™ okna `ROW_NUMBER() ... ORDER BY order_purchase_timestamp DESC`, aby przypisaÄ‡ do klienta zawsze **aktualny adres** (na podstawie ostatniego zamÃ³wienia), tworzÄ…c spÃ³jny "Golden Record".

### 4. CiÄ…gÅ‚oÅ›Ä‡ Czasowa (Date Spine)
Wymiar czasu `dim_date` nie powstaÅ‚ z danych transakcyjnych (co powodowaÅ‚oby luki w dniach bez sprzedaÅ¼y), lecz zostaÅ‚ wygenerowany algorytmicznie za pomocÄ… pakietu `dbt_utils`. Gwarantuje to poprawnoÅ›Ä‡ analiz typu "Running Total" czy "Year-over-Year".

---

## Model Danych

### Model Galaxy Schema
Projekt wykorzystuje architekturÄ™ **Konstelacji FaktÃ³w**, gdzie trzy tabele faktÃ³w wspÃ³Å‚dzielÄ… wymiary (*conformed dimensions*).

<div align="center">
  <img src="readme_images/dwh.png" alt="Architektura systemu" width="100%">
</div>

---

## Struktura Projektu

```bash
.
â”œâ”€â”€ dags/                   # Definicje DAG-Ã³w Airflow
â”‚   â””â”€â”€ olist_elt_dump_dag.py
â”œâ”€â”€ olist_dbt/              # Projekt transformacji dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/        # Modele poÅ›rednie (Source & Cleaning)
â”‚   â”‚   â””â”€â”€ marts/          # Modele biznesowe (Galaxy Schema)
â”‚   â”œâ”€â”€ seeds/              # Pliki statyczne (CSV)
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ scripts/                # Skrypty pomocnicze
â”‚   â”œâ”€â”€ run_demo.py         # Orkiestrator symulacji
â”‚   â”œâ”€â”€ simulate_production.py
â”‚   â””â”€â”€ validate_data.py
â”œâ”€â”€ docker-compose.yml      # Definicja infrastruktury
â”œâ”€â”€ requirements.txt        # ZaleÅ¼noÅ›ci Python
â””â”€â”€ README.md
```

---

## Instrukcja Uruchomienia

### Wymagania
* Docker & Docker Compose
* Python 3.10+

### Szybki Start

**1. Konfiguracja Åšrodowiska**
Skopiuj przykÅ‚adowy plik konfiguracyjny (zawiera domyÅ›lne hasÅ‚a dla Å›rodowiska deweloperskiego).
```bash
cp .env.example .env
```

**2. Start Infrastruktury**
Uruchom kontenery bazy danych i Airflow.
```bash
docker-compose up -d --build
```

**3. Uruchomienie Symulacji (Demo)**
Skrypt `run_demo.py` automatycznie:
1. Utworzy wirtualne Å›rodowisko (opcjonalnie).
2. Symuluje napÅ‚yw danych historycznych (miesiÄ…c po miesiÄ…cu).
3. Wyzwoli odpowiednie procesy w Airflow.

```bash
# Przygotowanie Å›rodowiska Python
python -m venv .venv
# Windows: .venv\Scripts\activate | Linux/Mac: source .venv/bin/activate
source .venv/bin/activate
pip install -r requirements.txt

# Start demo
python scripts/run_demo.py
```

---

## Roadmapa i Status

- [x] **Infrastruktura**: Dockerized Airflow & Postgres.
- [x] **Logika ELT**: Custom Python Operators z transakcyjnÄ… spÃ³jnoÅ›ciÄ….
- [x] **Transformacja**: Modele dbt Incremental & implementacja Galaxy Schema.
- [x] **Orkiestracja**: Architektura Event-driven poprzez Airflow API.
- [x] **QA**: Automatyczna weryfikacja danych (`validate_data.py`).
- [ ] **Dokumentacja**: Hosting `dbt docs`.
- [ ] **BI**: Dashboardy w Metabase/Superset.

<br>

<div align="center">

**Autor:** Jakub Major

</div>