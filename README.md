<div align="center">

# Olist E-commerce Data Pipeline

![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7%2B-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![dbt](https://img.shields.io/badge/dbt--Core-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker--Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![SQLFluff](https://img.shields.io/badge/SQLFluff-Expected_Quality-00C7B7?style=for-the-badge&logo=sql&logoColor=white)

<br>

**Kompletny, skalowalny system ELT (Extract, Load, Transform) symulujÄ…cy Å›rodowisko produkcyjne e-commerce.**

[Kontekst](#-kontekst-i-cele) â€¢
[Technologie](#-zastosowane-wzorce-i-technologie) â€¢
[Architektura](#-architektura-i-przepÅ‚yw-danych) â€¢
[Uruchomienie](#-instrukcja-uruchomienia)

</div>

---

## ðŸ’¡ Kontekst i Cele

**Problem:** Surowe dane Olist to rozproszone logi transakcyjne â€“ analiza przychodu, opÃ³ÅºnieÅ„ czy retencji wymaga Å‚Ä…czenia wielu tabel i jest nieefektywna w czasie rzeczywistym.

**RozwiÄ…zanie:** Zautomatyzowany potok danych przeksztaÅ‚cajÄ…cy surowe logi w czysty model **Galaxy Schema** (Konstelacja FaktÃ³w) w Hurtowni Danych.

### ðŸ“š Dokumentacja Projektu (Live)
Projekt posiada w peÅ‚ni wygenerowanÄ… dokumentacjÄ™ dbt, dostÄ™pnÄ… online:
ðŸ‘‰ **[Kliknij tutaj, aby zobaczyÄ‡ Data Lineage i SÅ‚ownik Danych](https://jakubmajor23.github.io/olist-data-pipeline/)**

### GÅ‚Ã³wne Cele
- **Single Source of Truth:** Centralizacja danych o ZamÃ³wieniach, PÅ‚atnoÅ›ciach i Produktach.
- **SkalowalnoÅ›Ä‡:** Åadowanie przyrostowe (Incremental Loading) dla obsÅ‚ugi rosnÄ…cego wolumenu danych.
- **JakoÅ›Ä‡ Danych:** IntegralnoÅ›Ä‡ referencyjna, brak duplikatÃ³w (IdempotentnoÅ›Ä‡) i testy `dbt`.

---

## Zastosowane Wzorce i Technologie

Projekt realizuje zasady inÅ¼ynierii danych (**Modern Data Stack**) poprzez:

| Obszar | Implementacja |
| :--- | :--- |
| **Orkiestracja** | **Event-Driven Airflow**: Wyzwalanie DAG-Ã³w przez REST API zaraz po pojawieniu siÄ™ nowych danych (symulacja). |
| **Modelowanie** | **Galaxy Schema**: Architektura Konstelacji FaktÃ³w (3 tabele faktÃ³w) eliminujÄ…ca problem *Fan-out* i iloczynu kartezjaÅ„skiego. |
| **Transformacja** | **dbt Core**: Modele zmaterializowane jako `incremental` oraz `table`, makra Jinja (DRY), testy jakoÅ›ci danych. |
| **JakoÅ›Ä‡ Kodu** | **SQLFluff**: Linter SQL zapewniajÄ…cy spÃ³jny styl kodu (zgodnie z plikiem `.sqlfluff`). |
| **Infrastruktura** | **Docker & Docker Compose**: PeÅ‚na konteneryzacja Airflow (z dbt) oraz bazy danych Postgres. |

---

## Architektura i PrzepÅ‚yw Danych

System zaprojektowano moduÅ‚owo, oddzielajÄ…c warstwÄ™ symulacji od wÅ‚aÅ›ciwego przetwarzania.

### Cykl Å»ycia Danych (End-to-End Flow)

Proces symuluje rzeczywiste dziaÅ‚anie hurtowni danych w trybie przyrostowym (Incremental Load):

1.  **Symulacja Transakcji (`simulate_production.py`):**
    Skrypt pobiera dane historyczne (miesiÄ…c po miesiÄ…cu) i Å‚aduje je do operacyjnej bazy danych (`postgres-olist-source`), zachowujÄ…c Å›cisÅ‚e wiÄ™zy integralnoÅ›ci (Klienci â†’ ZamÃ³wienia â†’ PÅ‚atnoÅ›ci).

2.  **Trigger API (`run_demo.py`):**
    Natychmiast po zaÅ‚adowaniu danych, orkiestrator wysyÅ‚a zapytanie POST do REST API Airflow, przekazujÄ…c `logical_date`. Pozwala to na precyzyjne przetworzenie tylko nowego wycinka czasu.

3.  **Extract & Load (Airflow DAG):**
    * **IdempotentnoÅ›Ä‡:** Przed zaÅ‚adowaniem, DAG usuwa z warstwy `raw_data` wszelkie dane dla przetwarzanego miesiÄ…ca, zapobiegajÄ…c duplikatom.
    * **Transfer:** Dane sÄ… przenoszone z bazy ÅºrÃ³dÅ‚owej do hurtowni (Raw Layer) przy uÅ¼yciu wydajnych silnikÃ³w SQLAlchemy.

4.  **Transformacja (dbt):**
    Airflow uruchamia kontener z dbt (`dbt run`). Dane surowe sÄ… czyszczone (Staging) i modelowane do postaci tabel faktÃ³w i wymiarÃ³w (Marts).

5.  **Walidacja (`validate_data.py`):**
    Na samym koÅ„cu symulacji (`run_demo.py`) uruchamiany jest skrypt QA. Weryfikuje on integralnoÅ›Ä‡ danych, porÃ³wnujÄ…c liczbÄ™ wierszy miÄ™dzy oryginalnymi plikami CSV a **bazÄ… ÅºrÃ³dÅ‚owÄ…**, aby potwierdziÄ‡, Å¼e symulacja przebiegÅ‚a bez utraty danych.

---

## ðŸ› ï¸ SzczegÃ³Å‚y Transformacji (dbt)

Warstwa transformacji zostaÅ‚a podzielona na dwa etapy zgodnie z dobrymi praktykami Analytics Engineering:

### Warstwa Staging (Raw -> Staging)
* Materializacja jako `incremental` dla duÅ¼ych tabel (ZamÃ³wienia, PÅ‚atnoÅ›ci) i `table` dla sÅ‚ownikÃ³w.
* Logika **Fail Fast**: Plik `dbt_project.yml` wymusza testy unikalnoÅ›ci kluczy podstawowych.

### Warstwa Marts (Staging -> Facts/Dims)
Model **Galaxy Schema** Å‚Ä…czy procesy biznesowe przez wspÃ³lne wymiary (*Conformed Dimensions*).

| Tabela FaktÃ³w | Opis i Logika |
| :--- | :--- |
| **fact_orders** | Centralna tabela transakcyjna. Agreguje wartoÅ›ci koszyka (`SUM(price)`), koszty dostawy oraz Å‚Ä…czy statusy zamÃ³wieÅ„ i recenzje w jeden widok analityczny. |
| **fact_sales_items** | Najbardziej granularna tabela (poziom produktu w koszyku). Pozwala na analizÄ™ sprzedaÅ¼y per Produkt (`product_id`) i Sprzedawca (`seller_id`). |
| **fact_payments** | Analiza przepÅ‚ywÃ³w pieniÄ™Å¼nych, typÃ³w pÅ‚atnoÅ›ci (karta, voucher) oraz rat (`payment_installments`). |

---

## ðŸŒŸ WaÅ¼ne RozwiÄ…zania Techniczne

Projekt implementuje zaawansowane wzorce inÅ¼ynierii danych, wykraczajÄ…ce poza standardowe kursy ETL.

### 1. WydajnoÅ›Ä‡ i Modelowanie (dbt)
* **Eliminacja Fan-out (Galaxy Schema):** Åšwiadoma decyzja o rozdzieleniu danych na 3 tabele faktÃ³w (`fact_orders`, `fact_sales_items`, `fact_payments`). Zapobiega to eksplozji kartezjaÅ„skiej (powielaniu wierszy) i bÅ‚Ä™dom w agregacji, ktÃ³re wystÄ…piÅ‚yby przy prÃ³bie zÅ‚Ä…czenia relacji *One-to-Many-to-Many* w jednej tabeli.
* **Surrogate Keys & Incremental Logic:** Tabela `stg__order_items` nie posiada natywnego klucza gÅ‚Ã³wnego. RozwiÄ…zano to poprzez wygenerowanie deterministycznego klucza zastÄ™pczego `MD5(order_id || '-' || item_id)` oraz zÅ‚Ä…czenie z tabelÄ… zamÃ³wieÅ„ w celu wydajnego Å‚adowania przyrostowego.
* **Ghost Records (Unknown Members):** ObsÅ‚uga brakujÄ…cych kluczy obcych (np. w `dim_products`). BÅ‚Ä™dne relacje sÄ… mapowane do sztucznego rekordu `MD5('unknown')`, co zapobiega utracie danych w raportach przy zÅ‚Ä…czeniach `INNER JOIN`.

### 2. Architektura i BezpieczeÅ„stwo
* **IdempotentnoÅ›Ä‡ Transakcyjna (Savepoints):** Customowy mechanizm w Airflow (`begin_nested()`) usuwa dane z okresu docelowego przed zaÅ‚adowaniem nowych. Gwarantuje to brak duplikatÃ³w i bezpieczny rollback w przypadku bÅ‚Ä™du.
* **Strategia Fail-Fast:** Pipeline celowo przerywa dziaÅ‚anie na starcie, jeÅ›li wykryje brak kluczowych zmiennych Å›rodowiskowych (haseÅ‚), zapobiegajÄ…c "cichemu" dziaÅ‚aniu na niebezpiecznych ustawieniach.
* **Symulacja IntegralnoÅ›ci:** Skrypt Å‚adujÄ…cy dane emuluje system transakcyjny, zachowujÄ…c kolejnoÅ›Ä‡ insertÃ³w zgodnÄ… z wiÄ™zami Foreign Keys.

### 3. JakoÅ›Ä‡ i Czyszczenie Danych
* **Data Repair (Imputacja):** W warstwie Staging zaimplementowano logikÄ™ naprawczÄ… â€“ brakujÄ…ce daty zatwierdzenia zamÃ³wienia (`order_approved_at`) sÄ… uzupeÅ‚niane datÄ… zakupu, z jednoczesnym dodaniem flagi audytowej `is_imputed`.
* **Walidacja Geograficzna (Data Enrichment):** System nie usuwa "sztywno" danych, lecz wzbogaca je o metadane jakoÅ›ciowe. Koordynaty leÅ¼Ä…ce poza obrysem Brazylii otrzymujÄ… flagÄ™ `is_valid_brazilian_location = False`.
* **Golden Record (Klienci):** Wymiar `dim_customers` wybiera najbardziej aktualny adres klienta, uÅ¼ywajÄ…c funkcji okna do deduplikacji zmian adresowych w czasie.

### 4. Konfiguracja i UÅ¼ytecznoÅ›Ä‡
* **Code as Configuration (Seeds):** SÅ‚owniki (np. statusy zamÃ³wieÅ„) sÄ… zarzÄ…dzane jako pliki `dbt seeds` (CSV) zamiast byÄ‡ "zaszyte" w kodzie SQL. UmoÅ¼liwia to analitykom biznesowym aktualizacjÄ™ reguÅ‚ bez ingerencji w kod inÅ¼ynierski.
* **Translation Layer:** Produkty sÄ… automatycznie tÅ‚umaczone i standaryzowane (PT -> EN) poprzez zÅ‚Ä…czenie z tabelÄ… sÅ‚ownikowÄ…, co uÅ‚atwia globalne raportowanie bez koniecznoÅ›ci skomplikowanych instrukcji `CASE WHEN`. 

---

## Model Danych

### Model Galaxy Schema
Projekt wykorzystuje architekturÄ™ **Konstelacji FaktÃ³w**, gdzie trzy tabele faktÃ³w wspÃ³Å‚dzielÄ… wymiary (*conformed dimensions*).

<div align="center">
  <img src="readme_images/dwh.png" alt="Architektura systemu" width="100%">
</div>

| Tabela FaktÃ³w | Opis i Logika |
| :--- | :--- |
| **fact_orders** | Centralna tabela transakcyjna. Agreguje wartoÅ›ci koszyka, koszty dostawy oraz Å‚Ä…czy statusy zamÃ³wieÅ„ i recenzje w jeden widok analityczny. |
| **fact_sales_items** | Najbardziej granularna tabela (poziom produktu w koszyku). Pozwala na analizÄ™ sprzedaÅ¼y per Produkt i Sprzedawca. |
| **fact_payments** | Analiza przepÅ‚ywÃ³w pieniÄ™Å¼nych, typÃ³w pÅ‚atnoÅ›ci (karta, voucher) oraz rat. |


### Kluczowe Decyzje Modelowe
* **Role-Playing Dimensions:** Wymiar `dim_date` Å‚Ä…czy siÄ™ z tabelÄ… `fact_orders` wielokrotnie. Pozwala to na jednoczesnÄ… analizÄ™ rÃ³Å¼nych zdarzeÅ„ cyklu Å¼ycia zamÃ³wienia (data zakupu, zatwierdzenia, wysyÅ‚ki, dostawy) przy uÅ¼yciu jednej fizycznej tabeli kalendarza.
* **Outrigger Dimension:** Wymiar `dim_geolocation` nie jest podpiÄ™ty bezpoÅ›rednio do tabel faktÃ³w, lecz do wymiarÃ³w `dim_customer` i `dim_seller`. Taka normalizacja redukuje redundancjÄ™ danych adresowych i zapewnia spÃ³jnoÅ›Ä‡ geograficznÄ….

---

## ðŸ“‚ Struktura Projektu

```bash
.
â”œâ”€â”€ dags/                   # Definicje DAG-Ã³w Airflow
â”‚   â””â”€â”€ olist_elt_dump_dag.py
â”œâ”€â”€ olist_dbt/              # Projekt transformacji dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/        # Modele poÅ›rednie (Source & Cleaning, Surrogate Keys)
â”‚   â”‚   â””â”€â”€ marts/          # Modele biznesowe (Galaxy Schema, Ghost Records)
â”‚   â”œâ”€â”€ seeds/              # Pliki statyczne (CSV - lookup tables)
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ scripts/                # Skrypty pomocnicze
â”‚   â”œâ”€â”€ run_demo.py         # Orkiestrator symulacji
â”‚   â”œâ”€â”€ simulate_production.py
â”‚   â””â”€â”€ validate_data.py    # Skrypt QA (Quality Assurance)
â”œâ”€â”€ docker-compose.yml      # Definicja infrastruktury
â”œâ”€â”€ .sqlfluff               # Konfiguracja lintera SQL
â”œâ”€â”€ requirements.txt        # ZaleÅ¼noÅ›ci Python
â””â”€â”€ README.md
```

---

## Instrukcja Uruchomienia

### Wymagania
* Docker & Docker Compose
* Python 3.10+

### Szybki Start

**1. Konfiguracja Åšrodowiska**
Skopiuj przykÅ‚adowy plik konfiguracyjny (zawiera domyÅ›lne hasÅ‚a dla Å›rodowiska deweloperskiego).
```bash
cp .env.example .env
```

**2. Start Infrastruktury**
Uruchom kontenery bazy danych i Airflow.
```bash
docker-compose up -d --build
```

**3. Uruchomienie Symulacji (Demo)**
Skrypt `run_demo.py` automatycznie:
1. Utworzy wirtualne Å›rodowisko (opcjonalnie).
2. Symuluje napÅ‚yw danych historycznych (miesiÄ…c po miesiÄ…cu).
3. Wyzwoli odpowiednie procesy w Airflow.

```bash
# Przygotowanie Å›rodowiska Python
python -m venv .venv

# Windows:
.\.venv\Scripts\activate
# Linux / macOS:
source .venv/bin/activate

pip install -r requirements.txt

# Start demo
python scripts/run_demo.py
```

---

## Roadmapa i Status

- [x] **Infrastruktura**: Dockerized Airflow & Postgres.
- [x] **Logika ELT**: Custom Python Operators z transakcyjnÄ… spÃ³jnoÅ›ciÄ….
- [x] **Transformacja**: Modele dbt Incremental & implementacja Galaxy Schema.
- [x] **Orkiestracja**: Architektura Event-driven poprzez Airflow API.
- [x] **QA**: Automatyczna weryfikacja danych (`validate_data.py`).
- [x] **Dokumentacja**: Hosting `dbt docs`.
- [ ] **BI**: Dashboardy Power BI.

<br>

<div align="center">

**Autor:** Jakub Major

</div>