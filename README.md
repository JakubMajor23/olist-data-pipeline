<div align="center">

# Olist E-commerce Data Pipeline

![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7%2B-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)
![dbt](https://img.shields.io/badge/dbt--Core-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker--Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)

<br>

**Kompletny, skalowalny system ELT (Extract, Load, Transform) symulujÄ…cy Å›rodowisko produkcyjne e-commerce.**

[Kontekst](#-kontekst-i-cele) â€¢
[Technologie](#-zastosowane-wzorce-i-technologie) â€¢
[Architektura](#-architektura-i-przepÅ‚yw-danych) â€¢
[Uruchomienie](#-instrukcja-uruchomienia)

</div>

---

## ğŸ’¡ Kontekst i Cele

**Problem:** Surowe dane Olist to rozproszone logi transakcyjne â€“ analiza przychodu, opÃ³ÅºnieÅ„ czy retencji wymaga Å‚Ä…czenia wielu tabel i jest nieefektywna w czasie rzeczywistym.

**RozwiÄ…zanie:** Zautomatyzowany potok danych przeksztaÅ‚cajÄ…cy surowe logi w czysty model **Galaxy Schema** (Konstelacja FaktÃ³w) w Hurtowni Danych.

### GÅ‚Ã³wne Cele
- **Single Source of Truth:** Centralizacja danych o ZamÃ³wieniach, PÅ‚atnoÅ›ciach i Produktach.
- **SkalowalnoÅ›Ä‡:** Åadowanie przyrostowe (Incremental Loading) dla obsÅ‚ugi rosnÄ…cego wolumenu danych.
- **JakoÅ›Ä‡ Danych:** IntegralnoÅ›Ä‡ referencyjna, brak duplikatÃ³w (IdempotentnoÅ›Ä‡) i testy `dbt`.

---

## Zastosowane Wzorce i Technologie

Projekt realizuje zasady inÅ¼ynierii danych (**Modern Data Stack**) poprzez:

| Obszar | Implementacja |
| :--- | :--- |
| **Orkiestracja** | **Event-Driven Airflow**: Wyzwalanie DAG-Ã³w przez REST API zaraz po pojawieniu siÄ™ nowych danych (symulacja). |
| **Modelowanie** | **Galaxy Schema**: Architektura Konstelacji FaktÃ³w (3 tabele faktÃ³w) eliminujÄ…ca problem *Fan-out* i iloczynu kartezjaÅ„skiego. |
| **Transformacja** | **dbt Core**: Modele zmaterializowane jako `incremental` oraz `table`, makra Jinja (DRY), testy jakoÅ›ci danych. |
| **JakoÅ›Ä‡ Kodu** | **SQLFluff**: Linter SQL zapewniajÄ…cy spÃ³jny styl kodu (zgodnie z plikiem `.sqlfluff`). |
| **Infrastruktura** | **Docker & Docker Compose**: PeÅ‚na konteneryzacja Airflow (z dbt) oraz bazy danych Postgres. |

---

## Architektura i PrzepÅ‚yw Danych

System zaprojektowano moduÅ‚owo, oddzielajÄ…c warstwÄ™ symulacji od wÅ‚aÅ›ciwego przetwarzania.

### Cykl Å»ycia Danych (End-to-End Flow)

---

## Model Danych i Wyzwania

### Model Galaxy Schema
Projekt wykorzystuje architekturÄ™ **Konstelacji FaktÃ³w**, gdzie trzy tabele faktÃ³w wspÃ³Å‚dzielÄ… wymiary (*conformed dimensions*).

<div align="center">
  <img src="readme_images/dwh.png" alt="Architektura systemu" width="100%">
</div>

---

## Struktura Projektu

```bash
.
â”œâ”€â”€ dags/                   # Definicje DAG-Ã³w Airflow
â”‚   â””â”€â”€ olist_elt_dump_dag.py
â”œâ”€â”€ olist_dbt/              # Projekt transformacji dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/        # Modele poÅ›rednie (Source & Cleaning)
â”‚   â”‚   â””â”€â”€ marts/          # Modele biznesowe (Galaxy Schema)
â”‚   â”œâ”€â”€ seeds/              # Pliki statyczne (CSV)
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ scripts/                # Skrypty pomocnicze
â”‚   â”œâ”€â”€ run_demo.py         # Orkiestrator symulacji
â”‚   â”œâ”€â”€ simulate_production.py
â”‚   â””â”€â”€ validate_data.py
â”œâ”€â”€ docker-compose.yml      # Definicja infrastruktury
â”œâ”€â”€ requirements.txt        # ZaleÅ¼noÅ›ci Python
â””â”€â”€ README.md
```

---

## Instrukcja Uruchomienia

### Wymagania
* Docker & Docker Compose
* Python 3.10+

### Szybki Start

**1. Konfiguracja Åšrodowiska**
Skopiuj przykÅ‚adowy plik konfiguracyjny (zawiera domyÅ›lne hasÅ‚a dla Å›rodowiska deweloperskiego).
```bash
cp .env.example .env
```

**2. Start Infrastruktury**
Uruchom kontenery bazy danych i Airflow.
```bash
docker-compose up -d --build
```

**3. Uruchomienie Symulacji (Demo)**
Skrypt `run_demo.py` automatycznie:
1. Utworzy wirtualne Å›rodowisko (opcjonalnie).
2. Symuluje napÅ‚yw danych historycznych (miesiÄ…c po miesiÄ…cu).
3. Wyzwoli odpowiednie procesy w Airflow.

```bash
# Przygotowanie Å›rodowiska Python
python -m venv .venv
# Windows: .venv\Scripts\activate | Linux/Mac: source .venv/bin/activate
source .venv/bin/activate
pip install -r requirements.txt

# Start demo
python scripts/run_demo.py
```

---

## Roadmapa i Status

- [x] **Infrastruktura**: Dockerized Airflow & Postgres.
- [x] **Logika ELT**: Custom Python Operators z transakcyjnÄ… spÃ³jnoÅ›ciÄ….
- [x] **Transformacja**: Modele dbt Incremental & implementacja Galaxy Schema.
- [x] **Orkiestracja**: Architektura Event-driven poprzez Airflow API.
- [x] **QA**: Automatyczna weryfikacja danych (`validate_data.py`).
- [ ] **Dokumentacja**: Hosting `dbt docs`.
- [ ] **BI**: Dashboardy w Metabase/Superset.

<br>

<div align="center">

**Autor:** Jakub Major

</div>